{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ee3fcd0",
   "metadata": {},
   "source": [
    "# 02 Pickle File Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727269a2",
   "metadata": {},
   "source": [
    "This notebook demonstrates the typical pipeline of extracting data from the pickle files converted from NWB files in notebook 01.\n",
    "\n",
    "It is assumed that you have already finished notebook 01 - if not please run all the cells in notebook 01."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a039ab",
   "metadata": {},
   "source": [
    "### 1. Extracting Daily Data from Pickle Files\n",
    "\n",
    "data_tools.py is provided for easier access to pkl files. We start by importing data_tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e558bbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting working directory to root to mimic usual usecase\n",
    "###############################################################\n",
    "import sys\n",
    "import os\n",
    "root_path = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "sys.path.insert(0, root_path)\n",
    "###############################################################\n",
    "\n",
    "from tutorial.utils import data_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775d51e8",
   "metadata": {},
   "source": [
    "data_tools.py contains two functions, extract_dates_from_filenames and load_day. Combining the two gives you daily data extracted from pkl files.\n",
    "\n",
    "We first extract all the dates from the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033b67bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../data_test'\n",
    "dates = data_tools.extract_dates_from_filenames(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ac8e20",
   "metadata": {},
   "source": [
    "Then data from one day can be extracted as follows. data_CO is the data from centre-out trials and data_RD is the data from random trials. Some days contain both trials, some other days contain one of the trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0aecfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_CO, data_RD = data_tools.load_day(dates[0], data_path)\n",
    "print(f\"Data Exist in CO: {True if data_CO is not None else False} RD: {True if data_RD is not None else False}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a872f903",
   "metadata": {},
   "source": [
    "This cell shows you what type of data is contained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc4746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_CO.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efc0a2b",
   "metadata": {},
   "source": [
    "### 2. Typical Pipeline for Doing Things to Data [TODO: change this name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecfee63",
   "metadata": {},
   "source": [
    "The following code demonstrates with a simple example on how data can be done with things. [TODO: change what im saying]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfb8bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Arbitrary variable to be accumulated\n",
    "df_target_pos = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'target_positions_co': [None] * len(dates),\n",
    "    'target_positions_rd': [None] * len(dates)\n",
    "})\n",
    "\n",
    "# Main loop for the pipeline\n",
    "for date in dates:\n",
    "    data_CO, data_RD = data_tools.load_day(date, data_path)\n",
    "    if data_CO and data_RD:\n",
    "        rd_pos = data_RD['target_positions']\n",
    "        co_pos = data_CO['target_positions']\n",
    "    elif data_RD:\n",
    "        rd_pos = data_RD['target_positions']\n",
    "        co_pos = None\n",
    "    else:\n",
    "        rd_pos = None\n",
    "        co_pos = data_CO['target_positions']\n",
    "    \n",
    "    # Replace the following code with your logic to populate your accumulation variable\n",
    "    df_target_pos.at[df_target_pos.index[df_target_pos['date'] == date][0], 'target_positions_co'] = co_pos\n",
    "    df_target_pos.at[df_target_pos.index[df_target_pos['date'] == date][0], 'target_positions_rd'] = rd_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9713bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tutorial.config import mpl_config\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Left: CO\n",
    "for pos in df_target_pos['target_positions_co']:\n",
    "    if pos is not None:\n",
    "        axes[0].scatter(pos[:, 0], pos[:, 1], alpha=0.5)\n",
    "axes[0].set_title('All CO Target Positions')\n",
    "axes[0].set_xlabel('X')\n",
    "axes[0].set_ylabel('Y')\n",
    "axes[0].axis('equal')\n",
    "\n",
    "# Right: RD\n",
    "for pos in df_target_pos['target_positions_rd']:\n",
    "    if pos is not None:\n",
    "        axes[1].scatter(pos[:, 0], pos[:, 1], alpha=0.5)\n",
    "axes[1].set_title('All RD Target Positions')\n",
    "axes[1].set_xlabel('X')\n",
    "axes[1].set_ylabel('Y')\n",
    "axes[1].axis('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
